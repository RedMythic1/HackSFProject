{"title": "Vision Now Available in Llama.cpp", "summary": "Vision Now Available in Llama.cpp\n\nThe Llama project now supports multimodal input via the llama.cpp library. Two tools currently support this feature, and users can enable it using one of two methods: either by default, the multimodal projector will be offloaded to the GPU, or by disabling it with the --no-mmproj-offload flag. These are ready-to-use models, most of which come with Q4_K_M quantization by default. Users can replace the (tool_name) placeholder with the name of the binary they want to use, such as llama-mtmd-cli or llama-server. Some models may require a large context window, such as -c 8192.", "timestamp": 1746860871.376628}